# Policy Gradients - Apprentissage par Renforcement

Ce projet implÃ©mente l'apprentissage par renforcement avec **Gradient de Politique** en utilisant l'algorithme du gradient de politique Monte-Carlo (**REINFORCE**).

## ğŸ“‹ Description

Dans ce projet, nous implÃ©mentons notre propre Gradient de Politique dans la boucle d'apprentissage par renforcement en utilisant l'algorithme du gradient de politique Monte-Carlo - Ã©galement appelÃ© REINFORCE. Cette mÃ©thode permet Ã  un agent d'apprendre une politique optimale en maximisant la rÃ©compense attendue grÃ¢ce Ã  l'optimisation basÃ©e sur le gradient.

### Concepts clÃ©s

- **Policy Gradient** : MÃ©thode d'optimisation directe de la politique
- **REINFORCE** : Algorithme Monte-Carlo pour l'estimation du gradient de politique
- **Baseline** : Technique pour rÃ©duire la variance des estimations

## ğŸ¯ Objectifs d'apprentissage

Ã€ la fin de ce projet, vous devriez Ãªtre capable d'expliquer :

- **Qu'est-ce qu'une Politique ?** Une fonction qui mappe les Ã©tats aux actions
- **Comment calculer un Gradient de Politique ?** Utilisation du thÃ©orÃ¨me du gradient de politique
- **Qu'est-ce que le gradient de politique Monte-Carlo ?** Estimation par Ã©chantillonnage Monte-Carlo
- **Comment implÃ©menter REINFORCE ?** Algorithme complet avec baseline optionnelle

## ğŸ› ï¸ PrÃ©requis

### Environnement systÃ¨me

- **OS** : Ubuntu 20.04 LTS
- **Python** : Version 3.9
- **Style** : Respect du standard `pycodestyle` (version 2.11.1)

### DÃ©pendances Python

```bash
numpy==1.25.2
gymnasium==0.29.1
```

### Standards de code

- Tous les modules, classes et fonctions doivent Ãªtre documentÃ©s
- Code conforme aux standards PEP 8
- Tests unitaires inclus

## ğŸ“ Structure du projet

```
policy_gradients/
â”œâ”€â”€ README.md                 # Documentation du projet
â”œâ”€â”€ policy_gradient.py        # ImplÃ©mentation du gradient de politique
â”œâ”€â”€ train.py                  # Fonction d'entraÃ®nement REINFORCE
â”œâ”€â”€ 0-main.py                 # Test de la fonction policy
â”œâ”€â”€ 1-main.py                 # Test du gradient de politique Monte-Carlo
â”œâ”€â”€ 2-main.py                 # Test de la fonction d'entraÃ®nement
â””â”€â”€ 3-main.py                 # Test avec animation de l'entraÃ®nement
```

### Description des fichiers

| Fichier              | Description                                                                                         |
| -------------------- | --------------------------------------------------------------------------------------------------- |
| `policy_gradient.py` | Contient les fonctions `policy` et `policy_gradient` pour l'implÃ©mentation du gradient de politique |
| `train.py`           | Contient la fonction d'entraÃ®nement complÃ¨te pour l'algorithme REINFORCE                            |
| `0-main.py`          | Script de test pour la fonction policy simple                                                       |
| `1-main.py`          | Script de test pour le gradient de politique Monte-Carlo                                            |
| `2-main.py`          | Script de test pour la fonction d'entraÃ®nement                                                      |
| `3-main.py`          | Script de test avec visualisation de l'entraÃ®nement                                                 |

## ğŸš€ Installation et utilisation

### Installation des dÃ©pendances

```bash
pip install python==3.9 numpy==1.25.2 gymnasium==0.29.1
```

### ExÃ©cution des tests

```bash
# Tester la fonction policy simple
./0-main.py

# Tester le gradient de politique Monte-Carlo
./1-main.py

# Tester la fonction d'entraÃ®nement
./2-main.py

# Tester l'entraÃ®nement avec animation
./3-main.py
```

### Utilisation des modules

```python
from policy_gradient import policy, policy_gradient
from train import train

# Exemple d'utilisation
# Voir les fichiers main pour des exemples dÃ©taillÃ©s
```

## ğŸ“Š Algorithme REINFORCE

L'algorithme REINFORCE suit ces Ã©tapes principales :

1. **Initialisation** : ParamÃ¨tres de politique Î¸
2. **Collecte de donnÃ©es** : GÃ©nÃ©ration d'Ã©pisodes selon Ï€_Î¸
3. **Calcul des retours** : Estimation Monte-Carlo des rÃ©compenses
4. **Mise Ã  jour** : Gradient ascent sur log Ï€_Î¸(a|s) \* G_t
5. **RÃ©pÃ©tition** : Jusqu'Ã  convergence

## ğŸ” RÃ©sultats attendus

- Convergence de la politique vers l'optimal
- AmÃ©lioration progressive des rÃ©compenses
- StabilitÃ© de l'apprentissage avec baseline

## ğŸ‘¨â€ğŸ’» Auteur

**Samuel VERSCHUEREN**

## ğŸ“„ Licence

Ce projet fait partie du curriculum Holberton School.
